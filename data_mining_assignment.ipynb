{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3UBpI3624g8MhJmShUUJC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidpromo/data_mining/blob/main/data_mining_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lr3mx8BExVak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e00014-35ee-40d7-c40e-d61c8a2a1a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 850 samples\n",
            "Test set size: 150 samples\n",
            "Input layer size: 27, output layer size: 10\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/sidpromo/data_mining/refs/heads/main/features_30_sec.csv'\n",
        "data = pd.read_csv(url)\n",
        "data = data.drop(columns=['filename'])\n",
        "\n",
        "selected_columns = [\n",
        "    'chroma_stft_mean', 'chroma_stft_var',  # Chroma features\n",
        "    'spectral_centroid_mean', 'spectral_bandwidth_mean', 'rolloff_mean', 'zero_crossing_rate_mean',  # Spectral features\n",
        "    'tempo',  # Tempo\n",
        "    'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean', 'mfcc4_mean', 'mfcc5_mean',  # MFCCs (first 5)\n",
        "    'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean', 'mfcc9_mean', 'mfcc10_mean',  # MFCCs (next 5)\n",
        "    'mfcc11_mean', 'mfcc12_mean', 'mfcc13_mean', 'mfcc14_mean', 'mfcc15_mean',  # MFCCs (next 5)\n",
        "    'mfcc16_mean', 'mfcc17_mean', 'mfcc18_mean', 'mfcc19_mean', 'mfcc20_mean'  # MFCCs (last 5)\n",
        "]\n",
        "# Shuffle the data\n",
        "shuffled_data = data.sample(frac=1, random_state=42).reset_index(drop=True)  # frac=1 means shuffle all rows\n",
        "output_count = len(set(shuffled_data['label']))\n",
        "\n",
        "# Split the dataset\n",
        "train_data = shuffled_data.iloc[:850]\n",
        "test_data = shuffled_data.iloc[850:1000]\n",
        "\n",
        "# label conv\n",
        "labels_train = train_data['label']\n",
        "labels_test = test_data['label']\n",
        "\n",
        "categories = list(set(shuffled_data['label']))\n",
        "\n",
        "labels_train_numeric = pd.Categorical(labels_train, categories=categories).codes\n",
        "labels_test_numeric = pd.Categorical(labels_test, categories=categories).codes\n",
        "\n",
        "# features\n",
        "features_train = train_data[selected_columns]\n",
        "features_test = test_data[selected_columns]\n",
        "\n",
        "# normalize features\n",
        "scaler = StandardScaler()\n",
        "features_train = pd.DataFrame(scaler.fit_transform(features_train), columns=features_train.columns)\n",
        "features_test = pd.DataFrame(scaler.transform(features_test), columns=features_test.columns)\n",
        "\n",
        "print(f\"Training set size: {len(features_train)} samples\")\n",
        "print(f\"Test set size: {len(features_test)} samples\")\n",
        "print(f\"Input layer size: {features_train.shape[1]}, output layer size: {output_count}\")\n",
        "\n",
        "class GenreRecognizer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GenreRecognizer, self).__init__()\n",
        "        input_size = features_train.shape[1]\n",
        "        nr_first_hidden_layer = 256\n",
        "        nr_second_hidden_layer = 128\n",
        "        self.fc1 = nn.Linear(input_size, nr_first_hidden_layer)\n",
        "        self.fc2 = nn.Linear(nr_first_hidden_layer, nr_second_hidden_layer)\n",
        "        self.out = nn.Linear(nr_second_hidden_layer, 10)\n",
        "        self.dropout = nn.Dropout(p=0.3)  # Drop 30% of the neurons during training\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # ReLU activation for first hidden layer\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x)) # ReLU activation for second hidden layer\n",
        "        x = self.dropout(x)\n",
        "        x = self.out(x) # Output layer (logits)\n",
        "        return x # CrossEntropyLoss will handle softmax internally\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(features_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(labels_train_numeric, dtype=torch.long)\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=32)\n",
        "\n",
        "net = GenreRecognizer()\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for multi-class classification\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, weight_decay=2e-4)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    running_n = 0\n",
        "\n",
        "    # training acc\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, targets in trainloader:\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        outputs = net(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_n += 1\n",
        "\n",
        "        # Calculate predictions and accuracy\n",
        "        _, predicted = torch.max(outputs, 1)  # Get class with highest score\n",
        "        correct_predictions += (predicted == targets).sum().item()  # Count correct predictions\n",
        "        total_samples += targets.size(0)  # Count total samples\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Loss: {running_loss / running_n:.4f}')\n",
        "    # training_accuracy = correct_predictions / total_samples * 100  # Training accuracy as a percentage\n",
        "    # print(f'Epoch {epoch + 1}, Loss: {running_loss / running_n:.4f}, Training Accuracy: {training_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7T6NWDi3BJ4",
        "outputId": "0c202830-02bb-4b7d-8191-e8931916b6d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.2951\n",
            "Epoch 2, Loss: 2.2658\n",
            "Epoch 3, Loss: 2.2191\n",
            "Epoch 4, Loss: 2.1777\n",
            "Epoch 5, Loss: 2.1375\n",
            "Epoch 6, Loss: 2.0842\n",
            "Epoch 7, Loss: 2.0381\n",
            "Epoch 8, Loss: 1.9850\n",
            "Epoch 9, Loss: 1.9572\n",
            "Epoch 10, Loss: 1.9110\n",
            "Epoch 11, Loss: 1.8694\n",
            "Epoch 12, Loss: 1.8395\n",
            "Epoch 13, Loss: 1.7999\n",
            "Epoch 14, Loss: 1.7550\n",
            "Epoch 15, Loss: 1.7324\n",
            "Epoch 16, Loss: 1.7101\n",
            "Epoch 17, Loss: 1.6915\n",
            "Epoch 18, Loss: 1.6777\n",
            "Epoch 19, Loss: 1.6523\n",
            "Epoch 20, Loss: 1.6470\n",
            "Epoch 21, Loss: 1.6081\n",
            "Epoch 22, Loss: 1.5939\n",
            "Epoch 23, Loss: 1.5770\n",
            "Epoch 24, Loss: 1.5678\n",
            "Epoch 25, Loss: 1.5703\n",
            "Epoch 26, Loss: 1.5440\n",
            "Epoch 27, Loss: 1.5470\n",
            "Epoch 28, Loss: 1.5207\n",
            "Epoch 29, Loss: 1.5254\n",
            "Epoch 30, Loss: 1.4862\n",
            "Epoch 31, Loss: 1.4703\n",
            "Epoch 32, Loss: 1.4867\n",
            "Epoch 33, Loss: 1.4687\n",
            "Epoch 34, Loss: 1.4515\n",
            "Epoch 35, Loss: 1.4509\n",
            "Epoch 36, Loss: 1.4502\n",
            "Epoch 37, Loss: 1.4285\n",
            "Epoch 38, Loss: 1.4224\n",
            "Epoch 39, Loss: 1.3994\n",
            "Epoch 40, Loss: 1.3884\n",
            "Epoch 41, Loss: 1.3879\n",
            "Epoch 42, Loss: 1.3723\n",
            "Epoch 43, Loss: 1.3599\n",
            "Epoch 44, Loss: 1.3586\n",
            "Epoch 45, Loss: 1.3473\n",
            "Epoch 46, Loss: 1.3302\n",
            "Epoch 47, Loss: 1.3332\n",
            "Epoch 48, Loss: 1.3255\n",
            "Epoch 49, Loss: 1.3218\n",
            "Epoch 50, Loss: 1.3133\n",
            "Epoch 51, Loss: 1.3006\n",
            "Epoch 52, Loss: 1.2910\n",
            "Epoch 53, Loss: 1.2746\n",
            "Epoch 54, Loss: 1.2708\n",
            "Epoch 55, Loss: 1.2314\n",
            "Epoch 56, Loss: 1.2633\n",
            "Epoch 57, Loss: 1.2573\n",
            "Epoch 58, Loss: 1.2434\n",
            "Epoch 59, Loss: 1.2517\n",
            "Epoch 60, Loss: 1.2288\n",
            "Epoch 61, Loss: 1.2234\n",
            "Epoch 62, Loss: 1.2285\n",
            "Epoch 63, Loss: 1.1809\n",
            "Epoch 64, Loss: 1.1768\n",
            "Epoch 65, Loss: 1.2019\n",
            "Epoch 66, Loss: 1.1889\n",
            "Epoch 67, Loss: 1.1877\n",
            "Epoch 68, Loss: 1.1553\n",
            "Epoch 69, Loss: 1.1716\n",
            "Epoch 70, Loss: 1.1717\n",
            "Epoch 71, Loss: 1.1737\n",
            "Epoch 72, Loss: 1.1575\n",
            "Epoch 73, Loss: 1.1391\n",
            "Epoch 74, Loss: 1.1720\n",
            "Epoch 75, Loss: 1.1755\n",
            "Epoch 76, Loss: 1.1446\n",
            "Epoch 77, Loss: 1.1398\n",
            "Epoch 78, Loss: 1.1167\n",
            "Epoch 79, Loss: 1.1199\n",
            "Epoch 80, Loss: 1.1257\n",
            "Epoch 81, Loss: 1.1154\n",
            "Epoch 82, Loss: 1.1008\n",
            "Epoch 83, Loss: 1.0962\n",
            "Epoch 84, Loss: 1.1042\n",
            "Epoch 85, Loss: 1.1000\n",
            "Epoch 86, Loss: 1.1005\n",
            "Epoch 87, Loss: 1.0682\n",
            "Epoch 88, Loss: 1.0788\n",
            "Epoch 89, Loss: 1.0673\n",
            "Epoch 90, Loss: 1.0637\n",
            "Epoch 91, Loss: 1.0766\n",
            "Epoch 92, Loss: 1.0769\n",
            "Epoch 93, Loss: 1.0656\n",
            "Epoch 94, Loss: 1.0437\n",
            "Epoch 95, Loss: 1.0586\n",
            "Epoch 96, Loss: 1.0848\n",
            "Epoch 97, Loss: 1.0252\n",
            "Epoch 98, Loss: 1.0232\n",
            "Epoch 99, Loss: 1.0330\n",
            "Epoch 100, Loss: 1.0369\n",
            "Epoch 101, Loss: 1.0485\n",
            "Epoch 102, Loss: 1.0101\n",
            "Epoch 103, Loss: 1.0255\n",
            "Epoch 104, Loss: 1.0088\n",
            "Epoch 105, Loss: 0.9759\n",
            "Epoch 106, Loss: 0.9931\n",
            "Epoch 107, Loss: 1.0004\n",
            "Epoch 108, Loss: 0.9803\n",
            "Epoch 109, Loss: 0.9978\n",
            "Epoch 110, Loss: 0.9989\n",
            "Epoch 111, Loss: 0.9881\n",
            "Epoch 112, Loss: 0.9889\n",
            "Epoch 113, Loss: 0.9705\n",
            "Epoch 114, Loss: 0.9508\n",
            "Epoch 115, Loss: 0.9715\n",
            "Epoch 116, Loss: 0.9539\n",
            "Epoch 117, Loss: 0.9574\n",
            "Epoch 118, Loss: 0.9487\n",
            "Epoch 119, Loss: 0.9859\n",
            "Epoch 120, Loss: 0.9439\n",
            "Epoch 121, Loss: 0.9361\n",
            "Epoch 122, Loss: 0.9672\n",
            "Epoch 123, Loss: 0.9315\n",
            "Epoch 124, Loss: 0.9463\n",
            "Epoch 125, Loss: 0.9445\n",
            "Epoch 126, Loss: 0.9357\n",
            "Epoch 127, Loss: 0.9338\n",
            "Epoch 128, Loss: 0.9223\n",
            "Epoch 129, Loss: 0.9360\n",
            "Epoch 130, Loss: 0.9100\n",
            "Epoch 131, Loss: 0.9139\n",
            "Epoch 132, Loss: 0.9173\n",
            "Epoch 133, Loss: 0.9077\n",
            "Epoch 134, Loss: 0.9218\n",
            "Epoch 135, Loss: 0.9064\n",
            "Epoch 136, Loss: 0.9229\n",
            "Epoch 137, Loss: 0.9228\n",
            "Epoch 138, Loss: 0.9009\n",
            "Epoch 139, Loss: 0.9292\n",
            "Epoch 140, Loss: 0.8914\n",
            "Epoch 141, Loss: 0.8994\n",
            "Epoch 142, Loss: 0.8947\n",
            "Epoch 143, Loss: 0.8817\n",
            "Epoch 144, Loss: 0.8826\n",
            "Epoch 145, Loss: 0.8848\n",
            "Epoch 146, Loss: 0.8870\n",
            "Epoch 147, Loss: 0.8723\n",
            "Epoch 148, Loss: 0.8735\n",
            "Epoch 149, Loss: 0.8818\n",
            "Epoch 150, Loss: 0.8484\n",
            "Epoch 151, Loss: 0.8471\n",
            "Epoch 152, Loss: 0.8513\n",
            "Epoch 153, Loss: 0.8709\n",
            "Epoch 154, Loss: 0.8632\n",
            "Epoch 155, Loss: 0.8291\n",
            "Epoch 156, Loss: 0.8233\n",
            "Epoch 157, Loss: 0.8119\n",
            "Epoch 158, Loss: 0.8478\n",
            "Epoch 159, Loss: 0.8206\n",
            "Epoch 160, Loss: 0.8696\n",
            "Epoch 161, Loss: 0.8220\n",
            "Epoch 162, Loss: 0.8427\n",
            "Epoch 163, Loss: 0.8214\n",
            "Epoch 164, Loss: 0.8449\n",
            "Epoch 165, Loss: 0.8169\n",
            "Epoch 166, Loss: 0.8190\n",
            "Epoch 167, Loss: 0.8252\n",
            "Epoch 168, Loss: 0.8201\n",
            "Epoch 169, Loss: 0.8135\n",
            "Epoch 170, Loss: 0.7901\n",
            "Epoch 171, Loss: 0.7847\n",
            "Epoch 172, Loss: 0.8191\n",
            "Epoch 173, Loss: 0.8221\n",
            "Epoch 174, Loss: 0.7804\n",
            "Epoch 175, Loss: 0.7749\n",
            "Epoch 176, Loss: 0.7897\n",
            "Epoch 177, Loss: 0.7933\n",
            "Epoch 178, Loss: 0.7678\n",
            "Epoch 179, Loss: 0.7853\n",
            "Epoch 180, Loss: 0.7665\n",
            "Epoch 181, Loss: 0.7782\n",
            "Epoch 182, Loss: 0.7973\n",
            "Epoch 183, Loss: 0.7661\n",
            "Epoch 184, Loss: 0.7482\n",
            "Epoch 185, Loss: 0.7735\n",
            "Epoch 186, Loss: 0.8100\n",
            "Epoch 187, Loss: 0.7905\n",
            "Epoch 188, Loss: 0.7514\n",
            "Epoch 189, Loss: 0.7591\n",
            "Epoch 190, Loss: 0.7565\n",
            "Epoch 191, Loss: 0.7597\n",
            "Epoch 192, Loss: 0.7675\n",
            "Epoch 193, Loss: 0.7662\n",
            "Epoch 194, Loss: 0.7798\n",
            "Epoch 195, Loss: 0.7666\n",
            "Epoch 196, Loss: 0.7548\n",
            "Epoch 197, Loss: 0.7390\n",
            "Epoch 198, Loss: 0.7521\n",
            "Epoch 199, Loss: 0.7646\n",
            "Epoch 200, Loss: 0.7225\n",
            "Epoch 201, Loss: 0.7401\n",
            "Epoch 202, Loss: 0.7414\n",
            "Epoch 203, Loss: 0.7192\n",
            "Epoch 204, Loss: 0.7449\n",
            "Epoch 205, Loss: 0.7224\n",
            "Epoch 206, Loss: 0.7264\n",
            "Epoch 207, Loss: 0.7142\n",
            "Epoch 208, Loss: 0.7163\n",
            "Epoch 209, Loss: 0.6958\n",
            "Epoch 210, Loss: 0.7272\n",
            "Epoch 211, Loss: 0.7198\n",
            "Epoch 212, Loss: 0.7112\n",
            "Epoch 213, Loss: 0.7116\n",
            "Epoch 214, Loss: 0.7092\n",
            "Epoch 215, Loss: 0.7002\n",
            "Epoch 216, Loss: 0.7091\n",
            "Epoch 217, Loss: 0.7089\n",
            "Epoch 218, Loss: 0.6908\n",
            "Epoch 219, Loss: 0.7053\n",
            "Epoch 220, Loss: 0.6980\n",
            "Epoch 221, Loss: 0.7130\n",
            "Epoch 222, Loss: 0.6850\n",
            "Epoch 223, Loss: 0.6973\n",
            "Epoch 224, Loss: 0.7183\n",
            "Epoch 225, Loss: 0.6828\n",
            "Epoch 226, Loss: 0.6830\n",
            "Epoch 227, Loss: 0.6888\n",
            "Epoch 228, Loss: 0.6747\n",
            "Epoch 229, Loss: 0.6618\n",
            "Epoch 230, Loss: 0.6970\n",
            "Epoch 231, Loss: 0.6885\n",
            "Epoch 232, Loss: 0.6974\n",
            "Epoch 233, Loss: 0.6579\n",
            "Epoch 234, Loss: 0.6764\n",
            "Epoch 235, Loss: 0.6919\n",
            "Epoch 236, Loss: 0.6789\n",
            "Epoch 237, Loss: 0.6951\n",
            "Epoch 238, Loss: 0.6754\n",
            "Epoch 239, Loss: 0.6802\n",
            "Epoch 240, Loss: 0.6579\n",
            "Epoch 241, Loss: 0.6909\n",
            "Epoch 242, Loss: 0.6902\n",
            "Epoch 243, Loss: 0.6828\n",
            "Epoch 244, Loss: 0.6635\n",
            "Epoch 245, Loss: 0.6696\n",
            "Epoch 246, Loss: 0.6587\n",
            "Epoch 247, Loss: 0.6647\n",
            "Epoch 248, Loss: 0.6585\n",
            "Epoch 249, Loss: 0.6772\n",
            "Epoch 250, Loss: 0.6694\n",
            "Epoch 251, Loss: 0.6646\n",
            "Epoch 252, Loss: 0.6632\n",
            "Epoch 253, Loss: 0.6748\n",
            "Epoch 254, Loss: 0.6269\n",
            "Epoch 255, Loss: 0.6438\n",
            "Epoch 256, Loss: 0.6393\n",
            "Epoch 257, Loss: 0.6537\n",
            "Epoch 258, Loss: 0.6304\n",
            "Epoch 259, Loss: 0.6418\n",
            "Epoch 260, Loss: 0.6420\n",
            "Epoch 261, Loss: 0.6409\n",
            "Epoch 262, Loss: 0.6076\n",
            "Epoch 263, Loss: 0.6512\n",
            "Epoch 264, Loss: 0.6245\n",
            "Epoch 265, Loss: 0.6354\n",
            "Epoch 266, Loss: 0.6352\n",
            "Epoch 267, Loss: 0.6125\n",
            "Epoch 268, Loss: 0.6286\n",
            "Epoch 269, Loss: 0.6203\n",
            "Epoch 270, Loss: 0.6332\n",
            "Epoch 271, Loss: 0.6115\n",
            "Epoch 272, Loss: 0.6255\n",
            "Epoch 273, Loss: 0.6229\n",
            "Epoch 274, Loss: 0.5984\n",
            "Epoch 275, Loss: 0.6064\n",
            "Epoch 276, Loss: 0.6073\n",
            "Epoch 277, Loss: 0.5870\n",
            "Epoch 278, Loss: 0.6264\n",
            "Epoch 279, Loss: 0.6114\n",
            "Epoch 280, Loss: 0.6204\n",
            "Epoch 281, Loss: 0.6209\n",
            "Epoch 282, Loss: 0.5938\n",
            "Epoch 283, Loss: 0.6098\n",
            "Epoch 284, Loss: 0.5804\n",
            "Epoch 285, Loss: 0.6184\n",
            "Epoch 286, Loss: 0.5861\n",
            "Epoch 287, Loss: 0.6090\n",
            "Epoch 288, Loss: 0.6007\n",
            "Epoch 289, Loss: 0.5892\n",
            "Epoch 290, Loss: 0.5882\n",
            "Epoch 291, Loss: 0.6067\n",
            "Epoch 292, Loss: 0.6052\n",
            "Epoch 293, Loss: 0.5813\n",
            "Epoch 294, Loss: 0.6040\n",
            "Epoch 295, Loss: 0.5923\n",
            "Epoch 296, Loss: 0.5666\n",
            "Epoch 297, Loss: 0.5863\n",
            "Epoch 298, Loss: 0.5719\n",
            "Epoch 299, Loss: 0.5784\n",
            "Epoch 300, Loss: 0.5719\n",
            "Epoch 301, Loss: 0.5837\n",
            "Epoch 302, Loss: 0.5644\n",
            "Epoch 303, Loss: 0.5754\n",
            "Epoch 304, Loss: 0.5611\n",
            "Epoch 305, Loss: 0.5575\n",
            "Epoch 306, Loss: 0.5751\n",
            "Epoch 307, Loss: 0.5627\n",
            "Epoch 308, Loss: 0.5662\n",
            "Epoch 309, Loss: 0.5496\n",
            "Epoch 310, Loss: 0.5216\n",
            "Epoch 311, Loss: 0.5504\n",
            "Epoch 312, Loss: 0.5510\n",
            "Epoch 313, Loss: 0.5572\n",
            "Epoch 314, Loss: 0.5887\n",
            "Epoch 315, Loss: 0.5623\n",
            "Epoch 316, Loss: 0.5684\n",
            "Epoch 317, Loss: 0.5434\n",
            "Epoch 318, Loss: 0.5322\n",
            "Epoch 319, Loss: 0.5525\n",
            "Epoch 320, Loss: 0.5453\n",
            "Epoch 321, Loss: 0.5536\n",
            "Epoch 322, Loss: 0.5357\n",
            "Epoch 323, Loss: 0.5847\n",
            "Epoch 324, Loss: 0.5396\n",
            "Epoch 325, Loss: 0.5472\n",
            "Epoch 326, Loss: 0.5399\n",
            "Epoch 327, Loss: 0.5567\n",
            "Epoch 328, Loss: 0.5746\n",
            "Epoch 329, Loss: 0.5300\n",
            "Epoch 330, Loss: 0.5580\n",
            "Epoch 331, Loss: 0.5281\n",
            "Epoch 332, Loss: 0.5132\n",
            "Epoch 333, Loss: 0.5394\n",
            "Epoch 334, Loss: 0.5286\n",
            "Epoch 335, Loss: 0.5492\n",
            "Epoch 336, Loss: 0.5684\n",
            "Epoch 337, Loss: 0.5321\n",
            "Epoch 338, Loss: 0.5089\n",
            "Epoch 339, Loss: 0.5001\n",
            "Epoch 340, Loss: 0.5341\n",
            "Epoch 341, Loss: 0.5190\n",
            "Epoch 342, Loss: 0.5091\n",
            "Epoch 343, Loss: 0.5125\n",
            "Epoch 344, Loss: 0.5215\n",
            "Epoch 345, Loss: 0.5355\n",
            "Epoch 346, Loss: 0.5230\n",
            "Epoch 347, Loss: 0.5463\n",
            "Epoch 348, Loss: 0.4987\n",
            "Epoch 349, Loss: 0.4776\n",
            "Epoch 350, Loss: 0.4871\n",
            "Epoch 351, Loss: 0.5051\n",
            "Epoch 352, Loss: 0.5123\n",
            "Epoch 353, Loss: 0.4879\n",
            "Epoch 354, Loss: 0.5366\n",
            "Epoch 355, Loss: 0.5170\n",
            "Epoch 356, Loss: 0.4879\n",
            "Epoch 357, Loss: 0.4919\n",
            "Epoch 358, Loss: 0.5058\n",
            "Epoch 359, Loss: 0.4946\n",
            "Epoch 360, Loss: 0.5130\n",
            "Epoch 361, Loss: 0.5246\n",
            "Epoch 362, Loss: 0.5046\n",
            "Epoch 363, Loss: 0.4877\n",
            "Epoch 364, Loss: 0.5014\n",
            "Epoch 365, Loss: 0.5064\n",
            "Epoch 366, Loss: 0.5107\n",
            "Epoch 367, Loss: 0.4875\n",
            "Epoch 368, Loss: 0.4914\n",
            "Epoch 369, Loss: 0.4850\n",
            "Epoch 370, Loss: 0.4741\n",
            "Epoch 371, Loss: 0.5121\n",
            "Epoch 372, Loss: 0.4670\n",
            "Epoch 373, Loss: 0.4735\n",
            "Epoch 374, Loss: 0.4946\n",
            "Epoch 375, Loss: 0.4855\n",
            "Epoch 376, Loss: 0.4708\n",
            "Epoch 377, Loss: 0.4897\n",
            "Epoch 378, Loss: 0.4835\n",
            "Epoch 379, Loss: 0.5074\n",
            "Epoch 380, Loss: 0.4689\n",
            "Epoch 381, Loss: 0.4736\n",
            "Epoch 382, Loss: 0.4763\n",
            "Epoch 383, Loss: 0.4581\n",
            "Epoch 384, Loss: 0.4573\n",
            "Epoch 385, Loss: 0.4628\n",
            "Epoch 386, Loss: 0.4674\n",
            "Epoch 387, Loss: 0.4924\n",
            "Epoch 388, Loss: 0.4637\n",
            "Epoch 389, Loss: 0.4926\n",
            "Epoch 390, Loss: 0.4473\n",
            "Epoch 391, Loss: 0.4659\n",
            "Epoch 392, Loss: 0.4591\n",
            "Epoch 393, Loss: 0.4497\n",
            "Epoch 394, Loss: 0.4643\n",
            "Epoch 395, Loss: 0.4817\n",
            "Epoch 396, Loss: 0.4521\n",
            "Epoch 397, Loss: 0.4553\n",
            "Epoch 398, Loss: 0.4629\n",
            "Epoch 399, Loss: 0.4359\n",
            "Epoch 400, Loss: 0.4633\n",
            "Epoch 401, Loss: 0.4303\n",
            "Epoch 402, Loss: 0.4769\n",
            "Epoch 403, Loss: 0.4355\n",
            "Epoch 404, Loss: 0.4709\n",
            "Epoch 405, Loss: 0.4376\n",
            "Epoch 406, Loss: 0.4224\n",
            "Epoch 407, Loss: 0.4219\n",
            "Epoch 408, Loss: 0.4526\n",
            "Epoch 409, Loss: 0.4404\n",
            "Epoch 410, Loss: 0.4436\n",
            "Epoch 411, Loss: 0.4391\n",
            "Epoch 412, Loss: 0.4146\n",
            "Epoch 413, Loss: 0.4522\n",
            "Epoch 414, Loss: 0.4458\n",
            "Epoch 415, Loss: 0.4219\n",
            "Epoch 416, Loss: 0.4191\n",
            "Epoch 417, Loss: 0.4150\n",
            "Epoch 418, Loss: 0.4460\n",
            "Epoch 419, Loss: 0.4387\n",
            "Epoch 420, Loss: 0.4370\n",
            "Epoch 421, Loss: 0.4667\n",
            "Epoch 422, Loss: 0.4558\n",
            "Epoch 423, Loss: 0.4215\n",
            "Epoch 424, Loss: 0.4126\n",
            "Epoch 425, Loss: 0.4349\n",
            "Epoch 426, Loss: 0.4312\n",
            "Epoch 427, Loss: 0.4228\n",
            "Epoch 428, Loss: 0.4517\n",
            "Epoch 429, Loss: 0.4101\n",
            "Epoch 430, Loss: 0.4233\n",
            "Epoch 431, Loss: 0.4414\n",
            "Epoch 432, Loss: 0.4188\n",
            "Epoch 433, Loss: 0.4323\n",
            "Epoch 434, Loss: 0.4133\n",
            "Epoch 435, Loss: 0.3939\n",
            "Epoch 436, Loss: 0.4070\n",
            "Epoch 437, Loss: 0.4250\n",
            "Epoch 438, Loss: 0.4381\n",
            "Epoch 439, Loss: 0.4247\n",
            "Epoch 440, Loss: 0.3955\n",
            "Epoch 441, Loss: 0.3973\n",
            "Epoch 442, Loss: 0.4263\n",
            "Epoch 443, Loss: 0.3924\n",
            "Epoch 444, Loss: 0.3963\n",
            "Epoch 445, Loss: 0.4029\n",
            "Epoch 446, Loss: 0.3959\n",
            "Epoch 447, Loss: 0.4053\n",
            "Epoch 448, Loss: 0.4114\n",
            "Epoch 449, Loss: 0.4055\n",
            "Epoch 450, Loss: 0.4167\n",
            "Epoch 451, Loss: 0.3950\n",
            "Epoch 452, Loss: 0.3876\n",
            "Epoch 453, Loss: 0.4046\n",
            "Epoch 454, Loss: 0.3905\n",
            "Epoch 455, Loss: 0.3913\n",
            "Epoch 456, Loss: 0.3855\n",
            "Epoch 457, Loss: 0.4046\n",
            "Epoch 458, Loss: 0.4242\n",
            "Epoch 459, Loss: 0.3786\n",
            "Epoch 460, Loss: 0.4236\n",
            "Epoch 461, Loss: 0.3952\n",
            "Epoch 462, Loss: 0.4154\n",
            "Epoch 463, Loss: 0.3830\n",
            "Epoch 464, Loss: 0.4254\n",
            "Epoch 465, Loss: 0.3968\n",
            "Epoch 466, Loss: 0.3772\n",
            "Epoch 467, Loss: 0.4111\n",
            "Epoch 468, Loss: 0.3791\n",
            "Epoch 469, Loss: 0.4100\n",
            "Epoch 470, Loss: 0.3995\n",
            "Epoch 471, Loss: 0.3764\n",
            "Epoch 472, Loss: 0.3874\n",
            "Epoch 473, Loss: 0.3720\n",
            "Epoch 474, Loss: 0.3951\n",
            "Epoch 475, Loss: 0.3877\n",
            "Epoch 476, Loss: 0.3952\n",
            "Epoch 477, Loss: 0.3833\n",
            "Epoch 478, Loss: 0.4072\n",
            "Epoch 479, Loss: 0.3796\n",
            "Epoch 480, Loss: 0.3804\n",
            "Epoch 481, Loss: 0.3903\n",
            "Epoch 482, Loss: 0.3683\n",
            "Epoch 483, Loss: 0.4074\n",
            "Epoch 484, Loss: 0.3927\n",
            "Epoch 485, Loss: 0.3719\n",
            "Epoch 486, Loss: 0.3620\n",
            "Epoch 487, Loss: 0.3667\n",
            "Epoch 488, Loss: 0.3592\n",
            "Epoch 489, Loss: 0.3663\n",
            "Epoch 490, Loss: 0.3703\n",
            "Epoch 491, Loss: 0.3706\n",
            "Epoch 492, Loss: 0.3570\n",
            "Epoch 493, Loss: 0.3483\n",
            "Epoch 494, Loss: 0.3586\n",
            "Epoch 495, Loss: 0.3737\n",
            "Epoch 496, Loss: 0.3861\n",
            "Epoch 497, Loss: 0.3806\n",
            "Epoch 498, Loss: 0.3613\n",
            "Epoch 499, Loss: 0.3607\n",
            "Epoch 500, Loss: 0.3618\n",
            "Epoch 501, Loss: 0.3552\n",
            "Epoch 502, Loss: 0.3413\n",
            "Epoch 503, Loss: 0.3575\n",
            "Epoch 504, Loss: 0.3513\n",
            "Epoch 505, Loss: 0.3583\n",
            "Epoch 506, Loss: 0.3593\n",
            "Epoch 507, Loss: 0.3298\n",
            "Epoch 508, Loss: 0.3796\n",
            "Epoch 509, Loss: 0.3898\n",
            "Epoch 510, Loss: 0.3641\n",
            "Epoch 511, Loss: 0.3294\n",
            "Epoch 512, Loss: 0.3630\n",
            "Epoch 513, Loss: 0.3375\n",
            "Epoch 514, Loss: 0.3386\n",
            "Epoch 515, Loss: 0.3599\n",
            "Epoch 516, Loss: 0.3560\n",
            "Epoch 517, Loss: 0.3641\n",
            "Epoch 518, Loss: 0.3331\n",
            "Epoch 519, Loss: 0.3474\n",
            "Epoch 520, Loss: 0.3468\n",
            "Epoch 521, Loss: 0.3397\n",
            "Epoch 522, Loss: 0.3396\n",
            "Epoch 523, Loss: 0.3568\n",
            "Epoch 524, Loss: 0.3209\n",
            "Epoch 525, Loss: 0.3347\n",
            "Epoch 526, Loss: 0.3580\n",
            "Epoch 527, Loss: 0.3520\n",
            "Epoch 528, Loss: 0.3538\n",
            "Epoch 529, Loss: 0.3514\n",
            "Epoch 530, Loss: 0.3566\n",
            "Epoch 531, Loss: 0.3395\n",
            "Epoch 532, Loss: 0.3374\n",
            "Epoch 533, Loss: 0.3369\n",
            "Epoch 534, Loss: 0.3109\n",
            "Epoch 535, Loss: 0.3132\n",
            "Epoch 536, Loss: 0.3355\n",
            "Epoch 537, Loss: 0.3524\n",
            "Epoch 538, Loss: 0.3339\n",
            "Epoch 539, Loss: 0.3228\n",
            "Epoch 540, Loss: 0.3387\n",
            "Epoch 541, Loss: 0.3379\n",
            "Epoch 542, Loss: 0.3343\n",
            "Epoch 543, Loss: 0.3458\n",
            "Epoch 544, Loss: 0.3275\n",
            "Epoch 545, Loss: 0.3175\n",
            "Epoch 546, Loss: 0.3327\n",
            "Epoch 547, Loss: 0.3224\n",
            "Epoch 548, Loss: 0.3406\n",
            "Epoch 549, Loss: 0.3411\n",
            "Epoch 550, Loss: 0.3152\n",
            "Epoch 551, Loss: 0.3162\n",
            "Epoch 552, Loss: 0.3320\n",
            "Epoch 553, Loss: 0.3199\n",
            "Epoch 554, Loss: 0.3289\n",
            "Epoch 555, Loss: 0.3195\n",
            "Epoch 556, Loss: 0.3361\n",
            "Epoch 557, Loss: 0.3292\n",
            "Epoch 558, Loss: 0.3429\n",
            "Epoch 559, Loss: 0.3199\n",
            "Epoch 560, Loss: 0.3058\n",
            "Epoch 561, Loss: 0.3130\n",
            "Epoch 562, Loss: 0.3152\n",
            "Epoch 563, Loss: 0.3027\n",
            "Epoch 564, Loss: 0.3209\n",
            "Epoch 565, Loss: 0.3236\n",
            "Epoch 566, Loss: 0.3040\n",
            "Epoch 567, Loss: 0.3081\n",
            "Epoch 568, Loss: 0.3236\n",
            "Epoch 569, Loss: 0.3002\n",
            "Epoch 570, Loss: 0.3336\n",
            "Epoch 571, Loss: 0.3052\n",
            "Epoch 572, Loss: 0.3179\n",
            "Epoch 573, Loss: 0.3157\n",
            "Epoch 574, Loss: 0.2933\n",
            "Epoch 575, Loss: 0.3216\n",
            "Epoch 576, Loss: 0.3067\n",
            "Epoch 577, Loss: 0.3046\n",
            "Epoch 578, Loss: 0.2989\n",
            "Epoch 579, Loss: 0.3059\n",
            "Epoch 580, Loss: 0.3290\n",
            "Epoch 581, Loss: 0.3370\n",
            "Epoch 582, Loss: 0.3334\n",
            "Epoch 583, Loss: 0.3007\n",
            "Epoch 584, Loss: 0.3200\n",
            "Epoch 585, Loss: 0.3000\n",
            "Epoch 586, Loss: 0.3077\n",
            "Epoch 587, Loss: 0.2629\n",
            "Epoch 588, Loss: 0.3292\n",
            "Epoch 589, Loss: 0.2801\n",
            "Epoch 590, Loss: 0.3127\n",
            "Epoch 591, Loss: 0.3137\n",
            "Epoch 592, Loss: 0.3145\n",
            "Epoch 593, Loss: 0.2946\n",
            "Epoch 594, Loss: 0.2914\n",
            "Epoch 595, Loss: 0.3058\n",
            "Epoch 596, Loss: 0.2892\n",
            "Epoch 597, Loss: 0.2824\n",
            "Epoch 598, Loss: 0.3240\n",
            "Epoch 599, Loss: 0.2892\n",
            "Epoch 600, Loss: 0.3010\n",
            "Epoch 601, Loss: 0.3258\n",
            "Epoch 602, Loss: 0.2991\n",
            "Epoch 603, Loss: 0.2821\n",
            "Epoch 604, Loss: 0.2704\n",
            "Epoch 605, Loss: 0.2723\n",
            "Epoch 606, Loss: 0.2711\n",
            "Epoch 607, Loss: 0.2896\n",
            "Epoch 608, Loss: 0.2944\n",
            "Epoch 609, Loss: 0.2597\n",
            "Epoch 610, Loss: 0.2831\n",
            "Epoch 611, Loss: 0.3073\n",
            "Epoch 612, Loss: 0.3071\n",
            "Epoch 613, Loss: 0.3016\n",
            "Epoch 614, Loss: 0.3116\n",
            "Epoch 615, Loss: 0.2789\n",
            "Epoch 616, Loss: 0.2631\n",
            "Epoch 617, Loss: 0.2903\n",
            "Epoch 618, Loss: 0.2878\n",
            "Epoch 619, Loss: 0.2766\n",
            "Epoch 620, Loss: 0.2741\n",
            "Epoch 621, Loss: 0.2985\n",
            "Epoch 622, Loss: 0.2887\n",
            "Epoch 623, Loss: 0.2680\n",
            "Epoch 624, Loss: 0.2806\n",
            "Epoch 625, Loss: 0.2647\n",
            "Epoch 626, Loss: 0.2866\n",
            "Epoch 627, Loss: 0.2988\n",
            "Epoch 628, Loss: 0.2789\n",
            "Epoch 629, Loss: 0.2873\n",
            "Epoch 630, Loss: 0.2712\n",
            "Epoch 631, Loss: 0.2682\n",
            "Epoch 632, Loss: 0.2616\n",
            "Epoch 633, Loss: 0.2799\n",
            "Epoch 634, Loss: 0.2645\n",
            "Epoch 635, Loss: 0.2540\n",
            "Epoch 636, Loss: 0.2691\n",
            "Epoch 637, Loss: 0.2708\n",
            "Epoch 638, Loss: 0.2670\n",
            "Epoch 639, Loss: 0.2668\n",
            "Epoch 640, Loss: 0.2602\n",
            "Epoch 641, Loss: 0.2702\n",
            "Epoch 642, Loss: 0.2860\n",
            "Epoch 643, Loss: 0.2540\n",
            "Epoch 644, Loss: 0.2924\n",
            "Epoch 645, Loss: 0.2659\n",
            "Epoch 646, Loss: 0.2823\n",
            "Epoch 647, Loss: 0.2661\n",
            "Epoch 648, Loss: 0.2657\n",
            "Epoch 649, Loss: 0.2568\n",
            "Epoch 650, Loss: 0.2572\n",
            "Epoch 651, Loss: 0.2766\n",
            "Epoch 652, Loss: 0.2647\n",
            "Epoch 653, Loss: 0.2692\n",
            "Epoch 654, Loss: 0.2806\n",
            "Epoch 655, Loss: 0.2750\n",
            "Epoch 656, Loss: 0.2754\n",
            "Epoch 657, Loss: 0.2756\n",
            "Epoch 658, Loss: 0.2649\n",
            "Epoch 659, Loss: 0.2541\n",
            "Epoch 660, Loss: 0.2643\n",
            "Epoch 661, Loss: 0.2597\n",
            "Epoch 662, Loss: 0.2551\n",
            "Epoch 663, Loss: 0.2536\n",
            "Epoch 664, Loss: 0.2710\n",
            "Epoch 665, Loss: 0.2656\n",
            "Epoch 666, Loss: 0.2859\n",
            "Epoch 667, Loss: 0.2567\n",
            "Epoch 668, Loss: 0.2700\n",
            "Epoch 669, Loss: 0.2214\n",
            "Epoch 670, Loss: 0.2484\n",
            "Epoch 671, Loss: 0.2700\n",
            "Epoch 672, Loss: 0.2528\n",
            "Epoch 673, Loss: 0.2587\n",
            "Epoch 674, Loss: 0.2408\n",
            "Epoch 675, Loss: 0.2588\n",
            "Epoch 676, Loss: 0.2601\n",
            "Epoch 677, Loss: 0.2659\n",
            "Epoch 678, Loss: 0.2423\n",
            "Epoch 679, Loss: 0.2571\n",
            "Epoch 680, Loss: 0.2604\n",
            "Epoch 681, Loss: 0.2389\n",
            "Epoch 682, Loss: 0.2492\n",
            "Epoch 683, Loss: 0.2578\n",
            "Epoch 684, Loss: 0.2394\n",
            "Epoch 685, Loss: 0.2507\n",
            "Epoch 686, Loss: 0.2604\n",
            "Epoch 687, Loss: 0.2674\n",
            "Epoch 688, Loss: 0.2798\n",
            "Epoch 689, Loss: 0.2465\n",
            "Epoch 690, Loss: 0.2386\n",
            "Epoch 691, Loss: 0.2341\n",
            "Epoch 692, Loss: 0.2487\n",
            "Epoch 693, Loss: 0.2408\n",
            "Epoch 694, Loss: 0.2417\n",
            "Epoch 695, Loss: 0.2515\n",
            "Epoch 696, Loss: 0.2460\n",
            "Epoch 697, Loss: 0.2499\n",
            "Epoch 698, Loss: 0.2514\n",
            "Epoch 699, Loss: 0.2314\n",
            "Epoch 700, Loss: 0.2509\n",
            "Epoch 701, Loss: 0.2624\n",
            "Epoch 702, Loss: 0.2438\n",
            "Epoch 703, Loss: 0.2285\n",
            "Epoch 704, Loss: 0.2487\n",
            "Epoch 705, Loss: 0.2443\n",
            "Epoch 706, Loss: 0.2312\n",
            "Epoch 707, Loss: 0.2293\n",
            "Epoch 708, Loss: 0.2414\n",
            "Epoch 709, Loss: 0.2354\n",
            "Epoch 710, Loss: 0.2295\n",
            "Epoch 711, Loss: 0.2336\n",
            "Epoch 712, Loss: 0.2487\n",
            "Epoch 713, Loss: 0.2565\n",
            "Epoch 714, Loss: 0.2327\n",
            "Epoch 715, Loss: 0.2257\n",
            "Epoch 716, Loss: 0.2303\n",
            "Epoch 717, Loss: 0.2245\n",
            "Epoch 718, Loss: 0.2264\n",
            "Epoch 719, Loss: 0.2459\n",
            "Epoch 720, Loss: 0.2369\n",
            "Epoch 721, Loss: 0.2306\n",
            "Epoch 722, Loss: 0.2176\n",
            "Epoch 723, Loss: 0.2397\n",
            "Epoch 724, Loss: 0.2126\n",
            "Epoch 725, Loss: 0.2405\n",
            "Epoch 726, Loss: 0.2412\n",
            "Epoch 727, Loss: 0.2093\n",
            "Epoch 728, Loss: 0.2290\n",
            "Epoch 729, Loss: 0.2393\n",
            "Epoch 730, Loss: 0.2199\n",
            "Epoch 731, Loss: 0.2450\n",
            "Epoch 732, Loss: 0.2439\n",
            "Epoch 733, Loss: 0.2272\n",
            "Epoch 734, Loss: 0.2472\n",
            "Epoch 735, Loss: 0.2158\n",
            "Epoch 736, Loss: 0.2416\n",
            "Epoch 737, Loss: 0.2367\n",
            "Epoch 738, Loss: 0.2144\n",
            "Epoch 739, Loss: 0.2227\n",
            "Epoch 740, Loss: 0.2100\n",
            "Epoch 741, Loss: 0.2202\n",
            "Epoch 742, Loss: 0.2135\n",
            "Epoch 743, Loss: 0.2176\n",
            "Epoch 744, Loss: 0.2134\n",
            "Epoch 745, Loss: 0.2409\n",
            "Epoch 746, Loss: 0.2242\n",
            "Epoch 747, Loss: 0.2304\n",
            "Epoch 748, Loss: 0.2175\n",
            "Epoch 749, Loss: 0.2278\n",
            "Epoch 750, Loss: 0.2193\n",
            "Epoch 751, Loss: 0.2291\n",
            "Epoch 752, Loss: 0.2258\n",
            "Epoch 753, Loss: 0.2106\n",
            "Epoch 754, Loss: 0.2447\n",
            "Epoch 755, Loss: 0.2196\n",
            "Epoch 756, Loss: 0.2076\n",
            "Epoch 757, Loss: 0.2316\n",
            "Epoch 758, Loss: 0.2290\n",
            "Epoch 759, Loss: 0.2349\n",
            "Epoch 760, Loss: 0.2118\n",
            "Epoch 761, Loss: 0.2158\n",
            "Epoch 762, Loss: 0.2243\n",
            "Epoch 763, Loss: 0.2267\n",
            "Epoch 764, Loss: 0.2032\n",
            "Epoch 765, Loss: 0.2238\n",
            "Epoch 766, Loss: 0.2175\n",
            "Epoch 767, Loss: 0.2232\n",
            "Epoch 768, Loss: 0.2125\n",
            "Epoch 769, Loss: 0.2085\n",
            "Epoch 770, Loss: 0.2108\n",
            "Epoch 771, Loss: 0.2336\n",
            "Epoch 772, Loss: 0.2010\n",
            "Epoch 773, Loss: 0.2151\n",
            "Epoch 774, Loss: 0.2182\n",
            "Epoch 775, Loss: 0.2091\n",
            "Epoch 776, Loss: 0.2124\n",
            "Epoch 777, Loss: 0.2136\n",
            "Epoch 778, Loss: 0.2102\n",
            "Epoch 779, Loss: 0.2064\n",
            "Epoch 780, Loss: 0.2092\n",
            "Epoch 781, Loss: 0.2114\n",
            "Epoch 782, Loss: 0.2208\n",
            "Epoch 783, Loss: 0.2133\n",
            "Epoch 784, Loss: 0.1928\n",
            "Epoch 785, Loss: 0.2139\n",
            "Epoch 786, Loss: 0.2177\n",
            "Epoch 787, Loss: 0.1866\n",
            "Epoch 788, Loss: 0.1964\n",
            "Epoch 789, Loss: 0.2086\n",
            "Epoch 790, Loss: 0.2022\n",
            "Epoch 791, Loss: 0.2097\n",
            "Epoch 792, Loss: 0.1820\n",
            "Epoch 793, Loss: 0.2208\n",
            "Epoch 794, Loss: 0.2144\n",
            "Epoch 795, Loss: 0.2054\n",
            "Epoch 796, Loss: 0.2175\n",
            "Epoch 797, Loss: 0.1943\n",
            "Epoch 798, Loss: 0.2153\n",
            "Epoch 799, Loss: 0.1910\n",
            "Epoch 800, Loss: 0.1863\n",
            "Epoch 801, Loss: 0.1971\n",
            "Epoch 802, Loss: 0.1917\n",
            "Epoch 803, Loss: 0.1915\n",
            "Epoch 804, Loss: 0.1877\n",
            "Epoch 805, Loss: 0.2135\n",
            "Epoch 806, Loss: 0.2120\n",
            "Epoch 807, Loss: 0.2125\n",
            "Epoch 808, Loss: 0.1860\n",
            "Epoch 809, Loss: 0.2295\n",
            "Epoch 810, Loss: 0.2050\n",
            "Epoch 811, Loss: 0.1908\n",
            "Epoch 812, Loss: 0.2241\n",
            "Epoch 813, Loss: 0.1925\n",
            "Epoch 814, Loss: 0.1902\n",
            "Epoch 815, Loss: 0.2057\n",
            "Epoch 816, Loss: 0.1733\n",
            "Epoch 817, Loss: 0.2090\n",
            "Epoch 818, Loss: 0.2017\n",
            "Epoch 819, Loss: 0.1701\n",
            "Epoch 820, Loss: 0.2040\n",
            "Epoch 821, Loss: 0.2121\n",
            "Epoch 822, Loss: 0.2013\n",
            "Epoch 823, Loss: 0.1786\n",
            "Epoch 824, Loss: 0.1909\n",
            "Epoch 825, Loss: 0.2061\n",
            "Epoch 826, Loss: 0.1870\n",
            "Epoch 827, Loss: 0.1680\n",
            "Epoch 828, Loss: 0.1916\n",
            "Epoch 829, Loss: 0.1863\n",
            "Epoch 830, Loss: 0.1738\n",
            "Epoch 831, Loss: 0.1960\n",
            "Epoch 832, Loss: 0.1907\n",
            "Epoch 833, Loss: 0.1734\n",
            "Epoch 834, Loss: 0.1810\n",
            "Epoch 835, Loss: 0.1927\n",
            "Epoch 836, Loss: 0.1731\n",
            "Epoch 837, Loss: 0.1883\n",
            "Epoch 838, Loss: 0.1973\n",
            "Epoch 839, Loss: 0.1922\n",
            "Epoch 840, Loss: 0.1838\n",
            "Epoch 841, Loss: 0.1934\n",
            "Epoch 842, Loss: 0.1808\n",
            "Epoch 843, Loss: 0.1944\n",
            "Epoch 844, Loss: 0.2084\n",
            "Epoch 845, Loss: 0.1683\n",
            "Epoch 846, Loss: 0.1932\n",
            "Epoch 847, Loss: 0.1874\n",
            "Epoch 848, Loss: 0.2001\n",
            "Epoch 849, Loss: 0.1869\n",
            "Epoch 850, Loss: 0.2051\n",
            "Epoch 851, Loss: 0.1946\n",
            "Epoch 852, Loss: 0.1922\n",
            "Epoch 853, Loss: 0.1910\n",
            "Epoch 854, Loss: 0.1834\n",
            "Epoch 855, Loss: 0.1753\n",
            "Epoch 856, Loss: 0.1964\n",
            "Epoch 857, Loss: 0.1918\n",
            "Epoch 858, Loss: 0.1946\n",
            "Epoch 859, Loss: 0.1746\n",
            "Epoch 860, Loss: 0.1982\n",
            "Epoch 861, Loss: 0.1861\n",
            "Epoch 862, Loss: 0.1892\n",
            "Epoch 863, Loss: 0.1808\n",
            "Epoch 864, Loss: 0.1798\n",
            "Epoch 865, Loss: 0.1871\n",
            "Epoch 866, Loss: 0.1763\n",
            "Epoch 867, Loss: 0.2012\n",
            "Epoch 868, Loss: 0.2035\n",
            "Epoch 869, Loss: 0.1865\n",
            "Epoch 870, Loss: 0.1737\n",
            "Epoch 871, Loss: 0.1786\n",
            "Epoch 872, Loss: 0.1874\n",
            "Epoch 873, Loss: 0.2007\n",
            "Epoch 874, Loss: 0.1774\n",
            "Epoch 875, Loss: 0.1735\n",
            "Epoch 876, Loss: 0.1773\n",
            "Epoch 877, Loss: 0.1831\n",
            "Epoch 878, Loss: 0.1562\n",
            "Epoch 879, Loss: 0.1684\n",
            "Epoch 880, Loss: 0.1714\n",
            "Epoch 881, Loss: 0.1863\n",
            "Epoch 882, Loss: 0.1748\n",
            "Epoch 883, Loss: 0.1687\n",
            "Epoch 884, Loss: 0.1676\n",
            "Epoch 885, Loss: 0.1743\n",
            "Epoch 886, Loss: 0.1880\n",
            "Epoch 887, Loss: 0.1677\n",
            "Epoch 888, Loss: 0.1734\n",
            "Epoch 889, Loss: 0.1821\n",
            "Epoch 890, Loss: 0.1786\n",
            "Epoch 891, Loss: 0.1653\n",
            "Epoch 892, Loss: 0.1979\n",
            "Epoch 893, Loss: 0.1812\n",
            "Epoch 894, Loss: 0.1900\n",
            "Epoch 895, Loss: 0.1888\n",
            "Epoch 896, Loss: 0.1848\n",
            "Epoch 897, Loss: 0.1780\n",
            "Epoch 898, Loss: 0.1642\n",
            "Epoch 899, Loss: 0.1891\n",
            "Epoch 900, Loss: 0.1815\n",
            "Epoch 901, Loss: 0.1781\n",
            "Epoch 902, Loss: 0.1674\n",
            "Epoch 903, Loss: 0.1602\n",
            "Epoch 904, Loss: 0.1705\n",
            "Epoch 905, Loss: 0.1851\n",
            "Epoch 906, Loss: 0.1590\n",
            "Epoch 907, Loss: 0.1843\n",
            "Epoch 908, Loss: 0.1669\n",
            "Epoch 909, Loss: 0.1804\n",
            "Epoch 910, Loss: 0.1477\n",
            "Epoch 911, Loss: 0.1698\n",
            "Epoch 912, Loss: 0.1793\n",
            "Epoch 913, Loss: 0.1595\n",
            "Epoch 914, Loss: 0.1826\n",
            "Epoch 915, Loss: 0.1701\n",
            "Epoch 916, Loss: 0.1551\n",
            "Epoch 917, Loss: 0.1729\n",
            "Epoch 918, Loss: 0.1633\n",
            "Epoch 919, Loss: 0.1905\n",
            "Epoch 920, Loss: 0.1605\n",
            "Epoch 921, Loss: 0.1592\n",
            "Epoch 922, Loss: 0.1579\n",
            "Epoch 923, Loss: 0.1471\n",
            "Epoch 924, Loss: 0.1587\n",
            "Epoch 925, Loss: 0.1588\n",
            "Epoch 926, Loss: 0.1671\n",
            "Epoch 927, Loss: 0.1599\n",
            "Epoch 928, Loss: 0.1695\n",
            "Epoch 929, Loss: 0.1739\n",
            "Epoch 930, Loss: 0.1652\n",
            "Epoch 931, Loss: 0.1757\n",
            "Epoch 932, Loss: 0.1745\n",
            "Epoch 933, Loss: 0.1606\n",
            "Epoch 934, Loss: 0.1647\n",
            "Epoch 935, Loss: 0.1541\n",
            "Epoch 936, Loss: 0.1561\n",
            "Epoch 937, Loss: 0.1628\n",
            "Epoch 938, Loss: 0.1708\n",
            "Epoch 939, Loss: 0.1728\n",
            "Epoch 940, Loss: 0.1615\n",
            "Epoch 941, Loss: 0.1540\n",
            "Epoch 942, Loss: 0.1615\n",
            "Epoch 943, Loss: 0.1604\n",
            "Epoch 944, Loss: 0.1643\n",
            "Epoch 945, Loss: 0.1674\n",
            "Epoch 946, Loss: 0.1903\n",
            "Epoch 947, Loss: 0.1507\n",
            "Epoch 948, Loss: 0.1700\n",
            "Epoch 949, Loss: 0.1636\n",
            "Epoch 950, Loss: 0.1787\n",
            "Epoch 951, Loss: 0.1593\n",
            "Epoch 952, Loss: 0.1530\n",
            "Epoch 953, Loss: 0.1666\n",
            "Epoch 954, Loss: 0.1728\n",
            "Epoch 955, Loss: 0.1473\n",
            "Epoch 956, Loss: 0.1678\n",
            "Epoch 957, Loss: 0.1583\n",
            "Epoch 958, Loss: 0.1496\n",
            "Epoch 959, Loss: 0.1720\n",
            "Epoch 960, Loss: 0.1616\n",
            "Epoch 961, Loss: 0.1726\n",
            "Epoch 962, Loss: 0.1677\n",
            "Epoch 963, Loss: 0.1496\n",
            "Epoch 964, Loss: 0.1808\n",
            "Epoch 965, Loss: 0.1514\n",
            "Epoch 966, Loss: 0.1566\n",
            "Epoch 967, Loss: 0.1619\n",
            "Epoch 968, Loss: 0.1478\n",
            "Epoch 969, Loss: 0.1587\n",
            "Epoch 970, Loss: 0.1637\n",
            "Epoch 971, Loss: 0.1451\n",
            "Epoch 972, Loss: 0.1628\n",
            "Epoch 973, Loss: 0.1647\n",
            "Epoch 974, Loss: 0.1648\n",
            "Epoch 975, Loss: 0.1567\n",
            "Epoch 976, Loss: 0.1506\n",
            "Epoch 977, Loss: 0.1780\n",
            "Epoch 978, Loss: 0.1598\n",
            "Epoch 979, Loss: 0.1575\n",
            "Epoch 980, Loss: 0.1591\n",
            "Epoch 981, Loss: 0.1882\n",
            "Epoch 982, Loss: 0.1666\n",
            "Epoch 983, Loss: 0.1788\n",
            "Epoch 984, Loss: 0.1518\n",
            "Epoch 985, Loss: 0.1486\n",
            "Epoch 986, Loss: 0.1448\n",
            "Epoch 987, Loss: 0.1677\n",
            "Epoch 988, Loss: 0.1428\n",
            "Epoch 989, Loss: 0.1574\n",
            "Epoch 990, Loss: 0.1723\n",
            "Epoch 991, Loss: 0.1682\n",
            "Epoch 992, Loss: 0.1548\n",
            "Epoch 993, Loss: 0.1611\n",
            "Epoch 994, Loss: 0.1730\n",
            "Epoch 995, Loss: 0.1435\n",
            "Epoch 996, Loss: 0.1462\n",
            "Epoch 997, Loss: 0.1325\n",
            "Epoch 998, Loss: 0.1427\n",
            "Epoch 999, Loss: 0.1513\n",
            "Epoch 1000, Loss: 0.1621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "net.eval()\n",
        "test_inputs = torch.tensor(features_test.values, dtype=torch.float32)\n",
        "test_targets = torch.tensor(labels_test_numeric, dtype=torch.short)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(test_inputs, test_targets)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for inputs, targets in testloader:\n",
        "    outputs = net(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += targets.size(0)\n",
        "    correct += (predicted == targets).sum().item()\n",
        "\n",
        "print(f\"Correct: {correct}, total: {total}, ration: {correct/total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjXcFJnKxFYv",
        "outputId": "93d90ce2-ade3-41b6-a60e-ba2e2c9462e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 107, total: 150, ration: 0.7133333333333334\n"
          ]
        }
      ]
    }
  ]
}